{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "\n",
    "\n",
    "rows = table.find_all(\"tr\")[1:]  \n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip()\n",
    "\n",
    "    \n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Artist: {artist}\")\n",
    "    print(f\"Upload Date: {upload_date}\")\n",
    "    print(f\"Views: {views}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "fixtures_link = driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']//a[contains(text(),'Fixtures')]\")\n",
    "fixtures_link.click()\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "fixtures = soup.find_all(\"div\", {\"class\": \"fixture__format-strip\"})\n",
    "\n",
    "for fixture in fixtures:\n",
    "    match_title = fixture.find(\"span\", {\"class\": \"u-unskewed-text fixture__format\"}).text.strip()\n",
    "    series = fixture.find(\"span\", {\"class\": \"u-unskewed-text fixture__tournament-label u-truncated\"}).text.strip()\n",
    "    place = fixture.find(\"p\", {\"class\": \"fixture__additional-info\"}).text.strip()\n",
    "    date = fixture.find(\"div\", {\"class\": \"fixture__full-date\"}).text.strip()\n",
    "    time = fixture.find(\"div\", {\"class\": \"fixture__date-details\"}).text.strip()\n",
    "\n",
    "    print(f\"Match Title: {match_title}\")\n",
    "    print(f\"Series: {series}\")\n",
    "    print(f\"Place: {place}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Time: {time}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "economy_link = soup.find(\"a\", text=\"Economy\")\n",
    "economy_url = url + economy_link[\"href\"]\n",
    "\n",
    "economy_response = requests.get(economy_url)\n",
    "economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "\n",
    "table = economy_soup.find(\"table\", {\"class\": \"display\"})\n",
    "\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gdp_18_19 = columns[2].text.strip()\n",
    "    gdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"GSDP(18-19): {gdp_18_19}\")\n",
    "    print(f\"GSDP(19-20): {gdp_19_20}\")\n",
    "    print(f\"Share(18-19): {share_18_19}\")\n",
    "    print(f\"GDP($ billion): {gdp_billion}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "trending_link = soup.find(\"a\", text=\"Trending\")\n",
    "trending_url = url + trending_link[\"href\"]\n",
    "\n",
    "trending_response = requests.get(trending_url)\n",
    "trending_soup = BeautifulSoup(trending_response.content, \"html.parser\")\n",
    "\n",
    "repositories = trending_soup.find_all(\"article\", {\"class\": \"Box-row\"})\n",
    "\n",
    "for repo in repositories:\n",
    "    title = repo.find(\"h1\", {\"class\": \"h3 lh-condensed\"}).text.strip()\n",
    "    description = repo.find(\"p\", {\"class\": \"col-9 color-text-secondary my-1 pr-4\"}).text.strip()\n",
    "    contributors = repo.find(\"a\", {\"class\": \"Link--muted d-inline-block mr-3\"}).text.strip()\n",
    "    language = repo.find(\"span\", {\"itemprop\": \"programmingLanguage\"}).text.strip()\n",
    "\n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Repository Description: {description}\")\n",
    "    print(f\"Contributors Count: {contributors}\")\n",
    "    print(f\"Language Used: {language}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a7cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "charts_link = soup.find(\"a\", text=\"Charts\")\n",
    "charts_url = url + charts_link[\"href\"]\n",
    "\n",
    "charts_response = requests.get(charts_url)\n",
    "charts_soup = BeautifulSoup(charts_response.content, \"html.parser\")\n",
    "\n",
    "hot_100_link = charts_soup.find(\"a\", text=\"Hot 100\")\n",
    "hot_100_url = url + hot_100_link[\"href\"]\n",
    "\n",
    "hot_100_response = requests.get(hot_100_url)\n",
    "hot_100_soup = BeautifulSoup(hot_100_response.content, \"html.parser\")\n",
    "\n",
    "songs = hot_100_soup.find_all(\"li\", {\"class\": \"chart-list__element\"})\n",
    "\n",
    "for song in songs:\n",
    "    song_name = song.find(\"span\", {\"class\": \"chart-element__information__song\"}).text.strip()\n",
    "    artist_name = song.find(\"span\", {\"class\": \"chart-element__information__artist\"}).text.strip()\n",
    "    last_week_rank = song.find(\"span\", {\"class\": \"chart-element__meta text--last\"}).text.strip()\n",
    "    peak_rank = song.find(\"span\", {\"class\": \"chart-element__meta text--peak\"}).text.strip()\n",
    "    weeks_on_board = song.find(\"span\", {\"class\": \"chart-element__meta text--week\"}).text.strip()\n",
    "\n",
    "    print(f\"Song Name: {song_name}\")\n",
    "    print(f\"Artist Name: {artist_name}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank}\")\n",
    "    print(f\"Peak Rank: {peak_rank}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", {\"class\": \"in-article sortable\"})\n",
    "\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_name = columns[0].text.strip()\n",
    "    author_name = columns[1].text.strip()\n",
    "    volumes_sold = columns[2].text.strip()\n",
    "    publisher = columns[3].text.strip()\n",
    "    genre = columns[4].text.strip()\n",
    "\n",
    "    print(f\"Book Name: {book_name}\")\n",
    "    print(f\"Author Name: {author_name}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold}\")\n",
    "    print(f\"Publisher: {publisher}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298dcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "series_list = soup.find(\"div\", {\"class\": \"lister-list\"})\n",
    "\n",
    "series = series_list.find_all(\"div\", {\"class\": \"lister-item-content\"})\n",
    "\n",
    "for item in series:\n",
    "    name = item.find(\"h3\", {\"class\": \"lister-item-header\"}).find(\"a\").text.strip()\n",
    "    year_span = item.find(\"span\", {\"class\": \"lister-item-year\"}).text.strip(\"()\")\n",
    "    genre = item.find(\"span\", {\"class\": \"genre\"}).text.strip()\n",
    "    run_time = item.find(\"span\", {\"class\": \"runtime\"}).text.strip()\n",
    "    ratings = item.find(\"span\", {\"class\": \"ipl-rating-star__rating\"}).text.strip()\n",
    "    votes = item.find(\"span\", {\"name\": \"nv\"}).text.strip()\n",
    "\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Year Span: {year_span}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"Run Time: {run_time}\")\n",
    "    print(f\"Ratings: {ratings}\")\n",
    "    print(f\"Votes: {votes}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "show_all_datasets_link = soup.find(\"a\", text=\"Show All 3861 Datasets\")\n",
    "show_all_datasets_url = url + show_all_datasets_link[\"href\"]\n",
    "\n",
    "datasets_response = requests.get(show_all_datasets_url)\n",
    "datasets_soup = BeautifulSoup(datasets_response.content, \"html.parser\")\n",
    "\n",
    "table = datasets_soup.find(\"table\", {\"cellpadding\": \"3\"})\n",
    "\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    instances = columns[4].text.strip()\n",
    "    attributes = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Attribute Type: {attribute_type}\")\n",
    "    print(f\"No. of Instances: {instances}\")\n",
    "    print(f\"No. of Attributes: {attributes}\")\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 9\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "url = \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "name_elements = driver.find_elements(By.XPATH, \"//span[@class='fl ellipsis']\")\n",
    "designation_elements = driver.find_elements(By.XPATH, \"//span[@class='ellipsis clr']\")\n",
    "company_elements = driver.find_elements(By.XPATH, \"//a[@class='ellipsis']//following-sibling::p[1]\")\n",
    "skills_elements = driver.find_elements(By.XPATH, \"//div[@class='hireSec highlightable']\")\n",
    "location_elements = driver.find_elements(By.XPATH, \"//small[@class='ellipsis']\")\n",
    "\n",
    "for name, designation, company, skills, location in zip(name_elements, designation_elements, company_elements, skills_elements, location_elements):\n",
    "    recruiter_name = name.text.strip()\n",
    "    recruiter_designation = designation.text.strip()\n",
    "    recruiter_company = company.text.strip()\n",
    "    recruiter_skills = skills.text.strip()\n",
    "    recruiter_location = location.text.strip()\n",
    "\n",
    "    print(f\"Name: {recruiter_name}\")\n",
    "    print(f\"Designation: {recruiter_designation}\")\n",
    "    print(f\"Company: {recruiter_company}\")\n",
    "    print(f\"Skills they Hire for: {recruiter_skills}\")\n",
    "    print(f\"Location: {recruiter_location}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
